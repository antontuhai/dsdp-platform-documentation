= Deploying the Platform from scratch in a public _AWS_ cloud environment
include::DSDP:ROOT:partial$templates/document-attributes/default-set-en.adoc[]

include::DSDP:ROOT:partial$admonitions/language-en.adoc[]

This guide provides detailed information on deploying the Platform from scratch in an _AWS_ environment, starting from creating an AWS account and ending with installing the Platform with all additional configurations.

== Prerequisites

Before deploying and configuring the platform, be sure to check the following artifacts.

=== Documentation artifacts

* [*] xref:release-notes:release-notes.adoc[]
* [*] xref:release-notes:breaking-changes.adoc[]
* [*] xref:update/overview.adoc[Updating] documents section -- needed only for the Platform updating procedure.

=== Creating an AWS account

Before installing OpenShift Container Platform on Amazon Web Services (AWS), you need to create an AWS account.

This can be done using the official documentation on the AWS website: https://aws.amazon.com/premiumsupport/knowledge-center/create-and-activate-aws-account/[How do I create and activate a new AWS account?]

=== Setting up an AWS account

Before installing OpenShift Container Platform, you need to configure your Amazon Web Services (AWS) account.

[#setup-route-53]
==== Setting up Route 53

To install OpenShift Container Platform, you need to register a domain. This can be done using the *Route 53* service, or you can use any other domain name registrar.

Also, the Amazon Web Services (AWS) account being used must have a dedicated public hosting zone in the Route 53 service.

TIP: Detailed description can be found in the official documentation on the OKD website: https://docs.openshift.com/container-platform/4.12/installing/installing_aws/installing-aws-account.html#installation-aws-route53_installing-aws-account[Configuring Route 53].

[#setup-external-domain]
==== Setting up an external domain

If a non-AWS Route 53 domain name registrar was used to create the domain, domain delegation must be performed. To do this, follow these steps:

* Go to the created AWS account and create a public hosting zone in the *Route 53* service (as described in p. xref:#setup-route-53[]). Name it the same as the created external domain.
* Enter the created public hosting zone and view the record of type *`NS`* (*Name Servers* -- these are the name servers that respond to DNS queries for the domain). The value will list the name servers. Save the names of these servers for use in subsequent steps.
* Go to the external domain name registrar where the domain was created.
* Open the settings of this domain and find the settings related to NS servers;
* Edit the NS servers according to the NS servers taken from the public hosting zone in the AWS account.

==== AWS account limits

The OpenShift Container Platform cluster uses several Amazon Web Services (AWS) components, and standard _service limits_ affect the cluster installation possibility.

A list of AWS components whose limits may affect the installation and operation of the OpenShift Container Platform cluster is provided in the documentation on the OKD website: https://docs.openshift.com/container-platform/4.12/installing/installing_aws/installing-aws-account.html#installation-aws-limits_installing-aws-account[AWS account limits].

NOTE: You must also increase the CPU limit for *_on-demand_* virtual machines in your Amazon Web Services (AWS) account. The necessary actions for this are described in the official documentation on the AWS website: https://aws.amazon.com/premiumsupport/knowledge-center/ec2-on-demand-instance-vcpu-increase/[How do I request an EC2 vCPU limit increase for my On-Demand Instance?]

==== Creating an IAM user

. Before installing OpenShift Container Platform, create an _IAM user_, using the official documentation on the AWS website: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html[Creating an IAM user in your AWS account].

. In addition, perform the following important requirements:

* Remove any *Service control policies (SCPs*) restrictions from the AWS account.
+
NOTE: During cluster creation, an associated AWS OpenID Connect (OIDC) identity provider is also created. This OIDC provider configuration is based on an open key located in the AWS region *`us-east-1`*. Clients with AWS SCP must allow the use of the AWS region *`us-east-1`* even if the cluster is deployed in another region. Without proper policy settings, permission errors may occur immediately, as the OKD installer checks the correctness of these settings.
+
TIP: Detailed information can be obtained in the official documentation, in section *1.1. DEPLOYMENT PREREQUISITES* of the https://access.redhat.com/documentation/en-us/red_hat_openshift_service_on_aws/4/pdf/prepare_your_environment/red_hat_openshift_service_on_aws-4-prepare_your_environment-en-us.pdf[Red Hat OpenShift Service on AWS 4. Prepare your environment] document.

* Correctly set the *_permissions boundary_* for the created IAM user.
+
Below is an example of a permissions boundary policy. You can use it or completely remove any permissions boundary.
+
[%collapsible]
._Example. Setting the permissions boundary_ policy
====
[source,json]
----
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "NotAction": [
                "iam:*"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "iam:Get*",
                "iam:List*",
                "iam:Tag*",
                "iam:Untag*",
                "iam:GenerateServiceLastAccessedDetails",
                "iam:GenerateCredentialReport",
                "iam:SimulateCustomPolicy",
                "iam:SimulatePrincipalPolicy",
                "iam:UploadSSHPublicKey",
                "iam:UpdateServerCertificate",
                "iam:CreateInstanceProfile",
                "iam:CreatePolicy",
                "iam:DeletePolicy",
                "iam:CreatePolicyVersion",
                "iam:DeletePolicyVersion",
                "iam:SetDefaultPolicyVersion",
                "iam:CreateServiceLinkedRole",
                "iam:DeleteServiceLinkedRole",
                "iam:CreateInstanceProfile",
                "iam:AddRoleToInstanceProfile",
                "iam:DeleteInstanceProfile",
                "iam:RemoveRoleFromInstanceProfile",
                "iam:UpdateRole",
                "iam:UpdateRoleDescription",
                "iam:DeleteRole",
                "iam:PassRole",
                "iam:DetachRolePolicy",
                "iam:DeleteRolePolicy",
                "iam:UpdateAssumeRolePolicy",
                "iam:CreateGroup",
                "iam:UpdateGroup",
                "iam:AddUserToGroup",
                "iam:RemoveUserFromGroup",
                "iam:PutGroupPolicy",
                "iam:DetachGroupPolicy",
                "iam:DetachUserPolicy",
                "iam:DeleteGroupPolicy",
                "iam:DeleteGroup",
                "iam:DeleteUserPolicy",
                "iam:AttachUserPolicy",
                "iam:AttachGroupPolicy",
                "iam:PutUserPolicy",
                "iam:DeleteUser",
                "iam:CreateRole",
                "iam:AttachRolePolicy",
                "iam:PutRolePermissionsBoundary",
                "iam:PutRolePolicy"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [


                "iam:CreateAccessKey",
                "iam:DeleteAccessKey",
                "iam:UpdateAccessKey",
                "iam:CreateLoginProfile",
                "iam:DeleteLoginProfile",
                "iam:UpdateLoginProfile",
                "iam:ChangePassword",
                "iam:CreateVirtualMFADevice",
                "iam:EnableMFADevice",
                "iam:ResyncMFADevice",
                "iam:DeleteVirtualMFADevice",
                "iam:DeactivateMFADevice",
                "iam:CreateServiceSpecificCredential",
                "iam:UpdateServiceSpecificCredential",
                "iam:ResetServiceSpecificCredential",
                "iam:DeleteServiceSpecificCredential"
            ],
            "Resource": "*"
        }
    ]
}
----
====

TIP: The process of creating an IAM user is described in detail in the official documentation on the OKD website: https://docs.openshift.com/container-platform/4.12/installing/installing_aws/installing-aws-account.html#installation-aws-iam-user_installing-aws-account[Creating an IAM user].

==== Required AWS permissions for the IAM user

To deploy all components of the OpenShift Container Platform cluster, the IAM user needs permissions, which must be attached to this user. +
An example of such permissions is provided in the documentation on the OKD website: https://docs.openshift.com/container-platform/4.12/installing/installing_aws/installing-aws-account.html#installation-aws-permissions_installing-aws-account[Required AWS permissions for the IAM user].

[#create-additional-accounts]
=== Creating additional accounts

Before installing OpenShift Container Platform on Amazon Web Services (AWS), you need to create a Docker Hub and Red Hat account. +
This is necessary to generate the *`docker pull secret`*, which will be used later.

==== Creating a Docker Hub account

* Some services use images located in Docker Hub repositories. To be able to use them, you need to create an account using the official documentation on the Docker website: https://docs.docker.com/docker-id/[Docker ID accounts].

* Additionally, you may encounter a limit on the number of image downloads per day. This will prevent services from starting. To avoid this, you need to upgrade your subscription to the Pro level. This will change the limit from 200 docker images/6 hours to 5000 docker images/day. This can be done using the official documentation on the Docker website: https://docs.docker.com/subscription/upgrade/[Upgrade your subscription].

==== Creating a Red Hat account

To download the necessary images for installing OpenShift Container Platform, you need to create a Red Hat Account. Detailed instructions on how to do this are described in the official documentation: https://access.redhat.com/articles/5832311[Red Hat Login ID and Account].

This is necessary to download the generated pull secret later (detailed in section xref:#okd-aws-install-preparation[]). It will allow authentication and downloading container images for OpenShift Container Platform components.

[#deploy-additional-recources-for-okd]
== Deploying additional resources for installing OKD cluster in AWS

To successfully install the cluster and platform, the following resources need to be deployed in AWS. The diagram below shows the infrastructure scheme with these resources. This is done to simplify the platform installation and avoid undesirable errors that may be associated with installing from a local computer.

image:installation/aws/installation-aws-1.svg[image,width=468,height=375]

=== Description of additional resources

A more detailed description of additional resources from the diagram is shown below:

* *S3 bucket*—used for storing the Terraform state;
* *DynamoDB table*—used for storing information about Terraform state locking;
* *NAT Gateway*—used to provide a private server with internet access;
* *Bastion*—used as an intermediate server to provide secure and restricted access to the server in the private network. Later an SSH tunnel will be created to the deployer-node through this bastion;
* *Deployer-node*—a server in the private network through which the cluster and Platform installation will take place.

These resources can be deployed using the prepared Terraform code in the following steps.

==== Recommended bastion settings

The table below shows the recommended settings for the bastion.

.Bastion settings
[width="100%",cols="6%,33%,61%",options="header",]
|===

|*No.* |*Setting option* |*Value*

|1 |Instance type |t2.nano
|2 |vCPUs |1
|3 |RAM |0.5 GiB
|4 |CPU Credits/hr |3
|5 |Platform |Ubuntu
|6 |AMI name |ubuntu-bionic-18.04-amd64-server-20210224
|7 |Volume |8 Gb

|===

==== Recommended deployer-node settings

The table below shows the recommended settings for the deployer-node.

.Deployer-node settings
[width="100%",cols="6%,33%,61%",options="header",]
|===

|*No.* |*Setting option* |*Value*
|1 |Instance type |t2.medium
|2 |vCPUs |2
|3 |RAM |4 GiB
|4 |CPU Credits/hr |24
|5 |Platform |Ubuntu
|6 |AMI name |ubuntu-bionic-18.04-amd64-server-20210224
|7 |Volume |150 Gb

|===

=== Additional settings

==== Installing the necessary tools

To proceed, you need to install the necessary tools on your local computer:

* unzip;
* https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html[aws cli v2];
* https://docs.docker.com/engine/install/[terraform v1.6.6].

You can check the correctness of the tool installation with the following commands:

.Checking tool installation
====

.Checking unzip
----
$ unzip -v
----

.Checking aws cli
----
$ aws --version
----

.Checking terraform
----
$ terraform version
----

====

==== Setting up AWS CLI

Authenticate in your AWS account using AWS CLI. To do this, execute the following command:

.*Authenticate in an AWS account*
[source,bash]
----
$ aws configure
AWS Access Key ID [None]: ********************
AWS Secret Access Key [None]: ***************************************
Default region name [None]: eu-central-1
Default output format [None]: json
----

TIP: The process of authenticating in an AWS account using AWS CLI is described in detail in the official documentation on the AWS website: https://docs.aws.amazon.com/cli/latest/userguide/cli-authentication-user.html#cli-authentication-user-configure.title[Configure the AWS CLI].

==== Setting up AWS cross-account

Before running the Terraform code, it needs to be downloaded. To do this, you need access to the AWS S3 bucket where it is located. This is possible only if a special IAM role is created. This can be done by following these steps:

. Create an AWS IAM role.
+
[source,bash]
----
$ aws iam create-role \
      --role-name UserCrossAccountRole \
      --description "Role for uploading terraform files from AWS S3" \
      --assume-role-policy-document '{
            "Version": "2012-10-17",
            "Statement": [
                {
                    "Action": "sts:AssumeRole",
                    "Effect": "Allow",
                    "Principal": {
                        "AWS": "arn:aws:iam::<YourAccountId>:root"
                    }
                }
            ]
          }'
----
+
[NOTE]
====
* *`<YourAccountId>`* -- add your AWS account ID here.
====

. Create an AWS IAM policy.
+
[source,bash]
----
$ aws iam create-policy \
      --policy-name UserCrossAccountPolicy \
      --policy-document '{
            "Version": "2012-10-17",
            "Statement": [
                {
                    "Action": "sts:AssumeRole",
                    "Effect": "Allow",
                    "Resource": "arn:aws:iam::764324427262:role/CustomCrossAccountRole"
                }
            ]
          }'
----

. Attach the policy to the role.
+
[source,bash]
----
$ aws iam attach-role-policy \
      --role-name UserCrossAccountRole \
      --policy-arn arn:aws:iam::<YourAccountId>:policy/UserCrossAccountPolicy
----
+
[NOTE]
====
* *`<YourAccountId>`* -- add your AWS account ID here.
====

. Add the necessary values for the role to the `config` file.
+
[source,bash]
----
$ cat <<EOT >> ~/.aws/config
[profile user-cross-account-role]
role_arn = arn:aws:iam::764324427262:role/CustomCrossAccountRole
source_profile = default
EOT
----

. To access files from an external AWS account, contact the support team. You need them to add your AWS account ID to the trusted list (trust relationship) for the `CustomCrossAccountRole` role in their AWS account.

==== Downloading Terraform code

. Download the archive with the Terraform code.
+
[source,bash]
----
$ aws s3 cp s3://mdtu-ddm-platform-installer/terraform/terraform.zip terraform.zip  --profile user-cross-account-role
----

. Unzip the Terraform code into a separate directory.
+
[source,bash]
----
$ unzip terraform.zip -d ~/terraform
----

=== Description of Terraform code

As an example of automating the process, Terraform code has been implemented, which can be customized and used to deploy the infrastructure.

==== Initial Terraform code

This is the Terraform code that will create resources for the subsequent steps. These resources include:

* S3 Bucket;
* DynamoDB Table.

Initial code. Description of Terraform files: ::

* `main.tf` -- the main Terraform configuration file. It contains modules for creating:
** S3 bucket;
** DynamoDB table.
* `providers.tf` -- used to define the version of Terraform, required plugins, and AWS provider parameters;
* `variables.tf` -- used to describe all variables used in the Terraform configuration;
* `terraform.tfvars` -- contains values for specific variables defined in the Terraform configuration files. If necessary, change the values for the following parameters to the required ones:
** `region` -- this variable is used to define the AWS region where the resources will be created;
** `tags` -- this variable is used to add tags (labels) for resources.

==== Main Terraform code

The main Terraform code deploys all the necessary resources. The template description is provided below.

.Main code. Description of Terraform files
* `main.tf` -- the main Terraform configuration file. It contains modules for creating:
** `VPC`;
** `ec2_bastion`;
** `ec2_instance`;
** `key_pair`.
* `providers.tf` -- used to define the version of Terraform, required plugins, and AWS provider parameters. Be sure to change the values for the following parameters to the required ones:
** `bucket` -- this variable contains the name of the S3 bucket. Change `<ACCOUNT_ID>` to the AWS account ID.
* `iam-node-role.tf` -- used to create a special IAM role with the necessary permissions. This will allow setting up AWS cross-account resource access and downloading Docker images for the container and Installer;
* `elastic-ip.tf` — used to create an AWS Elastic IP (EIP) resource using Terraform;
* `security-groups.tf` — creates Security Groups that allow SSH connections (TCP port 22) for bastion and deployer-node;
* `ssh-key.tf` — contains code for creating an SSH private key, saving the key to a file, and setting its access permissions;
* `files/user_data.sh.tpl` — a script template that will be executed when creating or updating an EC2 instance in the AWS environment. This script will do the following for the deployer-node:
** install Docker;
** install Unzip;
** install AWS CLI v2;
** additionally, set up AWS cross-account resource access.
* `variables.tf` — used to describe all variables used in the Terraform configuration;
* `terraform.tfvars` — contains values for specific variables defined in the Terraform configuration files. If necessary, change the values for the following parameters to the required ones:
** `region` -- this variable is used to define the AWS region where the resources will be created;
** `platform_name` — this variable is used to add the name for the cluster and AWS resources;
** `ingress_cidr_blocks` — to connect via SSH to the deployer-node, add the required IP address here;
** `prefix_list_ids` — if you need to open several addresses for connection, create a prefix-list and use its ID in this parameter;
** `tags` — this variable is used to add tags (labels) for resources.

=== Running the Terraform code

After making the changes in the previous steps, the Terraform code is now ready to run.

==== Running the initial Terraform code
. Sequentially execute the following commands to navigate to the directory with the initial Terraform code and initialize the Terraform working directory.
+
[source,bash]
----
$ cd ~/terraform/initCode

$ terraform init
----

. Use the following command to apply the changes defined in the configuration files and create the resources.
+
[source,bash]
----
$ terraform apply -auto-approve
----

. Wait for the resources to be created.

==== Running the main Terraform code
. Sequentially execute the following commands to navigate to the directory with the main Terraform code and initialize the Terraform working directory.
+
[source,bash]
----
$ cd ~/terraform/mainCode

$ terraform init
----

. Use the following command to apply the changes defined in the configuration files and create the resources.
+
[source,bash]
----
$ terraform apply -auto-approve
----

. Wait for the resources to be created.

=== Connecting to the deployer-node

To connect from a local computer to the deployer-node, you need to create an SSH tunnel. This should be done with the following command:

.Creating an SSH tunnel
====
----
$ ssh -i <SSH_KEY> -L 1256:<NODE_PRIVATE_IP>:22 -N -f ubuntu@<BASTION_PUBLIC_IP>
----
====

After creating the SSH tunnel, you can connect to the deployer-node. This should be done with the following command:

.Connecting via SSH
====
----
$ ssh -i <SSH_KEY> ubuntu@localhost -p 1256
----
====

[IMPORTANT]
====
Purpose of the deployer-node ::

All further steps, namely cluster installation and platform setup, should be performed from the deployer-node.
====

=== Running the openshift-install container

To use the docker image of the *`openshift-install`* container for installing the cluster, follow the steps below.

. Authenticate in AWS ECR.
+
[source,bash]
----
$ sudo aws ecr get-login-password --profile cross-account-role --region eu-central-1 | docker login --username AWS --password-stdin 764324427262.dkr.ecr.eu-central-1.amazonaws.com
----

. Download the docker image.
+
[source,bash]
----
$ docker pull 764324427262.dkr.ecr.eu-central-1.amazonaws.com/openshift-install:v3
----

. Add a tag to the downloaded docker image.
+
[source,bash]
----
$ docker tag 764324427262.dkr.ecr.eu-central-1.amazonaws.com/openshift-install:v3 openshift-install:v3
----

. Create a new directory to store all cluster data:
+
[source,bash]
----
$ mkdir ~/openshift-cluster
----

. Navigate to the created directory.
+
[source,bash]
----
$ cd ~/openshift-cluster
----

. Run the *`openshift-install`* container.
+
[source,bash]
----
$ sudo docker run --rm -it --name openshift-install-v3 \
    --user root:$(id -g) \
    --net host \
    -v $(pwd):/tmp/openshift-cluster \
    --env AWS_ACCESS_KEY_ID=<ACCESS_KEY> \
    --env AWS_SECRET_ACCESS_KEY=<SECRET_ACCESS_KEY> \
    openshift-install:v3 bash
----

[#okd-aws-install-preparation]
== Preparing for OKD cluster installation in AWS

In version `4.12` of OpenShift Container Platform, it is possible to install a customized cluster on infrastructure provided by the installer on Amazon Web Services (AWS).

[NOTE]
====
OKD version ::

The recommended OKD version is *`4.12.0-0.okd-2023-04-16-041331`*.
====

To install the cluster, perform the following steps:

. While in the container, navigate to the directory *_/tmp/openshift-cluster_*.
+
[source,bash]
----
$ cd /tmp/openshift-cluster
----

. Follow the actions described in the official documentation on the OKD website https://docs.openshift.com/container-platform/4.12/installing/installing_aws/installing-aws-customizations.html[Installing a cluster on AWS with customizations], up to the step *Obtaining an AWS Marketplace image*: https://docs.openshift.com/container-platform/4.12/installing/installing_aws/installing-aws-customizations.html#installation-aws-marketplace-subscribe_installing-aws-customizations[Obtaining an AWS Marketplace image]. +

. Download the OKD installer from the official GitHub repository.
+
[source,bash]
----
$ wget https://github.com/okd-project/okd/releases/download/4.12.0-0.okd-2023-04-16-041331/openshift-install-linux-4.12.0-0.okd-2023-04-16-041331.tar.gz
----

. Extract the installer from the downloaded archive.
+
[source,bash]
----
$ tar xvfz openshift-install-linux-4.12.0-0.okd-2023-04-16-041331.tar.gz
----
+
[CAUTION]
To configure the installation, you need to create an *_install-config.yaml_* file and enter the necessary parameters before installing the cluster.

. Create a new directory for cluster configuration files and the install-config.yaml file. To do this, execute the following commands in sequence:
+
[source,bash]
----
$ mkdir /tmp/openshift-cluster/cluster-state

$ touch /tmp/openshift-cluster/cluster-state/install-config.yaml
----
+
After creating the file, you need to fill it with the required parameters. The created configuration file includes only the necessary parameters for minimal cluster deployment. For customization settings, refer to the official documentation on the OKD website: https://docs.openshift.com/container-platform/4.12/installing/installing_aws/installing-aws-customizations.html#installation-initializing_installing-aws-customizations[Creating the installation configuration file].
+
Recommended parameters for the *_install-config.yaml_* file: ::
+
[%collapsible]
.*_install-config.yaml_*
====
[source,yaml]
----
apiVersion: v1
baseDomain: <BASE_DOMAIN> <1>
compute:
  - architecture: amd64
    hyperthreading: Enabled
    name: worker
    platform:
      aws:
        zones:
          - eu-central-1c
        rootVolume:
          size: 80
          type: gp3
        type: r5.2xlarge
        amiID: ami-06dac8f4521e7ec39
    replicas: 3
controlPlane:
  architecture: amd64
  hyperthreading: Enabled
  name: master
  platform:
    aws:
      zones:
        - eu-central-1c
      rootVolume:
        size: 80
        type: gp3
      type: r5.2xlarge
      amiID: ami-06dac8f4521e7ec39
  replicas: 3
metadata:
  name: <CLUSTER_NAME> <2>
networking:
  clusterNetwork:
    - cidr: 10.128.0.0/14
      hostPrefix: 23
  machineNetwork:
    - cidr: 10.0.0.0/16
  networkType: OVNKubernetes
platform:
  aws:
    region: eu-central-1
    userTags:
      'user:tag': <CLUSTER_NAME> <2>
publish: External
pullSecret: <PULL_SECRET> <4>
sshKey: <SSHKEY> <3>
----

<1> `<BASE_DOMAIN`> — the domain that was created and configured in sections xref:#setup-route-53[] and xref:#setup-external-domain[];

<2> `<CLUSTER_NAME>` — the name of the future OKD cluster;

<3> `<SSHKEY>` — the SSH key or keys for authenticating access to the cluster machines. You can use the same key created during the OKD cluster setup or any other key;
+
TIP: Detailed description can be found in the official documentation on the OKD website: https://docs.openshift.com/container-platform/4.12/installing/installing_aws/installing-aws-customizations.html#installation-configuration-parameters-optional_installing-aws-customizations[Optional configuration parameters].

<4> <PULL_SECRET> — the secret that was created in section xref:#create-additional-accounts[]. You need to obtain this secret from Red Hat OpenShift Cluster Manager.
+
TIP: A detailed description is provided in section 5 of the official documentation on the OKD website: https://docs.openshift.com/container-platform/4.12/installing/installing_aws/installing-aws-customizations.html#installation-obtaining-installer_installing-aws-customizations[Obtaining the installation program].
+
You also need to add the secret for connecting to the Red Hat account and the secret from the Docker Hub account to the obtained secret. The combined secret will look as follows:
+
._Example of a combined secret (pull secret)_
[%collapsible]
=====
[source,json]
----
{
   "auths":{
      "cloud.openshift.com":{
         "auth":"b3Blb=",
         "email":"test@example.com"
      },
      "quay.io":{
         "auth":"b3Blb=",
         "email":"test@example.com"
      },
      "registry.connect.redhat.com":{
         "username":"test",
         "password":"test",
         "auth":"b3Blb=",
         "email":"test@example.com"
      },
      "registry.redhat.io":{
         "username":"test",
         "password":"test",
         "auth":"b3Blb=",
         "email":"test@example.com"
      },
      "index.docker.io/v2/":{
         "username":"test",
         "password":"test",
         "auth":"b3Blb=",
         "email":"test@example.com"
      }
   }
}
----
=====
+
For convenience, write this secret in one line in the *_install-config.yaml_* file. The final secret will look as follows:
+
._Example of a one-line *pull secret*_
[%collapsible]
=====
----
'{"auths":{"cloud.openshift.com":{"auth":"b3Blb=","email":"test@example.com"},"quay.io":{"auth":"b3Blb=","email":"test@example.com"},"registry.connect.redhat.com":{"username":"test","password":"test","auth":"b3Blb=","email":"test@example.com"},"registry.redhat.io":{"username":"test","password":"test","auth":"b3Blb=","email":"test@example.com"},"index.docker.io/v2/":{"username":"test","password":"test","auth":"b3Blb=","email":"test@example.com"}}}'
----
=====

====
+
WARNING: After starting the cluster deployment process, the Installer deletes *install-config.yaml*, so it is recommended to back up this file if there is a need to deploy multiple clusters.

== Running the OKD4 installer and deploying an empty OKD4 cluster

After creating the *_install-config.yaml_* file, to deploy the OKD cluster, execute the following command:

.*Installing the OKD cluster*
[source,bash]
----
$ ./openshift-install create cluster --dir /tmp/openshift-cluster/cluster-state --log-level=info
----

NOTE: The cluster deployment process usually takes up to 1 hour.

Upon successful deployment, the following cluster access parameters will be provided as a result of the command execution:

* login;
* password;
* link to the cluster web console.

image:installation/aws/installation-aws-2.png[image,width=468,height=198]

In the directory where the command was executed, a number of files will be created that store the cluster status necessary for its uninstallation.

TIP: Detailed information is described in the official documentation on the OKD website, in the *Prerequisites* section: https://docs.openshift.com/container-platform/4.12/installing/installing_aws/uninstalling-cluster-aws.html#installation-uninstall-clouds_uninstall-cluster-aws[Uninstalling a cluster on AWS].

Also, in this directory, a *_/auth_* folder will appear, containing two files for authentication: for working with the cluster through the *web console* and the OKD *command line interface* (OKD CLI).

== Replacing self-signed certificates with trusted certificates

To replace self-signed certificates with trusted ones, you first need to obtain these certificates.

This section discusses obtaining free certificates from https://letsencrypt.org/[Let’s Encrypt] and installing them on the server.

Let’s Encrypt certificates are obtained using the https://github.com/acmesh-official/acme.sh[acme.sh] utility.

TIP: For details on using Let’s Encrypt based on the ACME protocol, refer to the https://letsencrypt.org/docs/client-options/[official source].

To replace the certificates, perform the following steps: ::
+
. Set an environment variable. The variable should point to the *_kubeconfig_* file.
+
[source,bash]
----
$ export KUBECONFIG=cluster-state/auth/kubeconfig
----

. Create a *_letsencrypt.sh_* file and insert the script below:
+
._Script for replacing certificates_
[%collapsible]
====
[source,bash]
----
#!/bin/bash
yum install -y openssl
mkdir -p certificates
export CERT_HOME=./certificates
export CURDIR=$(pwd)
cd $CERT_HOME

# Clone the acme.sh utility from the GitHub repository
git clone https://github.com/neilpang/acme.sh
sed -i "2i AWS_ACCESS_KEY_ID=\"${AWS_ACCESS_KEY_ID}\"" ./acme.sh/dnsapi/dns_aws.sh
sed -i "3i AWS_SECRET_ACCESS_KEY=\"${AWS_SECRET_ACCESS_KEY}\"" ./acme.sh/dnsapi/dns_aws.sh
cd $CURDIR
# Obtain the API Endpoint URL
export LE_API="$(oc whoami --show-server | cut -f 2 -d ':' | cut -f 3 -d '/' | sed 's/-api././')"
# Obtain the Wildcard Domain
export LE_WILDCARD="$(oc get ingresscontroller default -n openshift-ingress-operator -o jsonpath='{.status.domain}')"
${CERT_HOME}/acme.sh/acme.sh --register-account -m user_${RANDOM}@example.com
${CERT_HOME}/acme.sh/acme.sh --issue -d ${LE_API} -d *.${LE_WILDCARD} --dns dns_aws
export CERTDIR=$CERT_HOME/certificates
mkdir -p ${CERTDIR}

# Move certificates from the default acme.sh path to a more convenient directory using the --install-cert key
${CERT_HOME}/acme.sh/acme.sh --install-cert -d ${LE_API} -d *.${LE_WILDCARD} --cert-file ${CERTDIR}/cert.pem --key-file ${CERTDIR}/key.pem --fullchain-file ${CERTDIR}/fullchain.pem --ca-file ${CERTDIR}/ca.cer
# Create a secret
oc create secret tls router-certs --cert=${CERTDIR}/fullchain.pem --key=${CERTDIR}/key.pem -n openshift-ingress
# Update the Custom Resource for Router
oc patch ingresscontroller default -n openshift-ingress-operator --type=merge --patch='{"spec": { "defaultCertificate": { "name": "router-certs" }}}'
----
====

. Make this script executable.
+
[source,bash]
----
$ chmod +x ./letsencrypt.sh
----

. Run this script.
+
[source,bash]
----
$ bash -x ./letsencrypt.sh
----

. Exit the container after the script is executed. This can be done with the command below. The container will be deleted automatically.
+
.Exiting the container
----
$ exit
----

== Preparing and running the Installer to deploy and update the Platform in the OKD cluster

To run the _Installer_, several conditions must be met to prepare the workstation from which the Installer will be launched.

=== Deployment from scratch

==== Prerequisites

Before running the platform installation script, perform the following steps:

. Download the appropriate version of the Installer by executing the following commands in sequence.
+
[source,bash]
----
$ mkdir ~/installer

$ cd ~/installer

$ sudo aws s3 cp --profile cross-account-role s3://mdtu-ddm-platform-installer/<VERSION>/mdtu-ddm-platform-<VERSION>.tar.gz mdtu-ddm-platform-<VERSION>.tar.gz
----

. Unpack the Installer into a separate directory.
+
[source,bash]
----
$ mkdir -p ./installer-<VERSION>

$ tar -xvzf mdtu-ddm-platform-<VERSION>.tar.gz -C ./installer-<VERSION>
----

. Copy the *_kubeconfig_* from the installed cluster.
+
[source,bash]
----
$ cp ~/openshift-cluster/cluster-state/auth/kubeconfig ./installer-<VERSION>
----

==== Minio settings

No additional settings for Minio are required during the initial deployment of the Platform.

==== Vault settings

No additional settings for Vault are required during the initial deployment of the Platform.

[#deploy-platform-installer-scratch]
==== Deploying the Platform from the Installer

. Execute the following commands:
+
[source,bash]
----
$ IMAGE_CHECKSUM=$(sudo docker load -i control-plane-installer.img | sed -r "s#.*sha256:(.*)#\1#" | tr -d '\n')
----
+
[source,bash]
----
$ echo $IMAGE_CHECKSUM
----
+
[source,bash]
----
$ sudo docker tag ${IMAGE_CHECKSUM} control-plane-installer:<VERSION>
----

. Run the process of installing the new Platform with the images:
+
[source,bash]
----
$ sudo docker run --rm \ <1>
    --name control-plane-installer-<VERSION> \
    --user root:$(id -g) \
    --net host \
    -v $(pwd):/tmp/installer \
    --env KUBECONFIG=/tmp/installer/kubeconfig \
    --env PLATFORM_DEPLOYMENT_MODE=development \ <2>
    --env PLATFORM_LANGUAGE=en \ <3>
    --env PLATFORM_REGION=global \ <4>
    --entrypoint "/bin/sh" control-plane-installer:<VERSION> \
    -c "./install.sh -i" <5>
----
+
[TIP]
====
<1> *`--rm`* -- this parameter will automatically delete the container after its work is completed. The parameter can be removed if you need to check the status and log of the completed container or if you have an unstable internet connection;

<2> *`PLATFORM_DEPLOYMENT_MODE`*:
** *`development`* -- for deployment in development mode;
** *`production`* -- for deployment in production environment;

<3> *`PLATFORM_LANGUAGE`* -- an optional parameter which indicates the language to be used for the web portals and the Control Plane interface. Has a default value of *`en`* and can be changed after the Platform is deployed in the Control Plane and the Administrative Portals. Supported values:
** *`en`* -- indicates the use of English.
** *`uk`* -- indicates the use of Ukrainian.

<4> *`-i`* -- attribute indicates the installation of the Platform.
====

==== Deployment status

The final log shown below indicates the successful completion of the Platform update process:

image:admin:installation/aws/installation-aws-3.png[image,width=468,height=178]

If in section xref:#deploy-platform-installer-scratch[] the *`--rm`* option was removed, you need to: ::
+
. Execute the following command to ensure the container exited with status 0 (a container status indicating it successfully completed its work).
+
[source,bash]
----
$ docker ps --all --latest
----
+
image:admin:installation/aws/installation-aws-4.png[image,width=468,height=26]

. Delete the container with the following command:
+
[source,bash]
----
$ docker rm $(docker ps --latest -q)
----

==== Necessary steps after deployment

. After the Platform is installed, check that the *`cluster-management`* pipeline has started and ensure it has passed successfully (has a green status). [.underline]#_After this, the Platform will be ready for deploying registries. Without this action, registries will not be deployed_#.
+
The *`cluster-management`* pipeline can be found by the following path:
+
_menu:OKD Web UI[control-plane NS > Routes > jenkins url > cluster-mgmt > MASTER-Build-cluster-mgmt]_.

. Request access to the IIT widget, specifically https://eu.iit.com.ua/sign-widget/v20200922/.

[NOTE]
====
State of additional resources ::

After completing all actions, the bastion and deployer-node can be turned off. They will not be needed until the next Platform update.
====

[#installer-update]
=== Update

==== Prerequisites

Before running the platform installation script, perform the following steps:

. Download the appropriate version of the Installer by executing the following commands in sequence.
+
[source,bash]
----
$ mkdir ~/installer

$ cd ~/installer

$ sudo aws s3 cp --profile cross-account-role s3://mdtu-ddm-platform-installer/<VERSION>/mdtu-ddm-platform-<VERSION>.tar.gz mdtu-ddm-platform-<VERSION>.tar.gz
----

. Unpack the Installer into a separate directory.
+
[source,bash]
----
$ mkdir -p ./installer-<VERSION>

$ tar -xvzf mdtu-ddm-platform-<VERSION>.tar.gz -C ./installer-<VERSION>
----

. Copy the *_kubeconfig_* from the installed cluster.
+
----
$ cp ~/openshift-cluster/cluster-state/auth/kubeconfig ./installer-<VERSION>
----

==== Minio settings

. Copy the Minio terraform state from the previous release.
+
[source,bash]
----
$ cp ~/installer/installer-<VERSION>/terraform/minio/aws/terraform.tfstate ./terraform/minio/aws/
----

. Copy the Minio key from the previous release.
+
[source,bash]
----
$ cp ~/installer/installer-<VERSION>/terraform/minio/aws/private_minio.key ./terraform/minio/aws/
----

[#platform-update-vault]
==== Vault settings

. Copy the Vault terraform state from the previous release.
+
[source,bash]
----
$ cp ~/installer/installer-<VERSION>/terraform/vault/aws/terraform.tfstate ./terraform/vault/aws/
----

. Copy the Vault key from the previous release.
+
[source,bash]
----
$ cp ~/installer/installer-<VERSION>/terraform/vault/aws/private.key ./terraform/vault/aws/
----

[#update-platform-installer]
==== Updating the platform from the Installer

. Execute the following commands:
+
[source,bash]
----
$ IMAGE_CHECKSUM=$(sudo docker load -i control-plane-installer.img | sed -r "s#.*sha256:(.*)#\1#" | tr -d '\n')
----
+
[source,bash]
----
$ echo $IMAGE_CHECKSUM
----
+
[source,bash]
----
$ sudo docker tag ${IMAGE_CHECKSUM} control-plane-installer:<VERSION>
----

. Update the platform version with the images
+
[source,bash]
----
$ sudo docker run --rm \ <1>
    --name control-plane-installer-<VERSION> \
    --user root:$(id -g) \
    --net host \
    -v $(pwd):/tmp/installer \
    --env KUBECONFIG=/tmp/installer/kubeconfig \
    --env PLATFORM_DEPLOYMENT_MODE=development \ <2>
    --env PLATFORM_LANGUAGE=en \ <3>
    --entrypoint "/bin/sh" control-plane-installer:<VERSION> \
    -c "./install.sh -u" <4>
----
+
[TIP]
====
<1> *`--rm`* -- this parameter will automatically delete the container after its work is completed. This parameter can be removed if you need to check the status and log of the completed container or if you have an unstable internet connection;

<2> *`PLATFORM_DEPLOYMENT_MODE`*:
** *`development`* -- for deployment in development mode;
** *`production`* -- for deployment in production environment;

<3> *`PLATFORM_LANGUAGE`* -- an optional parameter which indicates the language to be used for the web portals and the Control Plane interface. Has a default value of *`en`* and can be changed after the Platform is deployed in the Control Plane and the Administrative Portals. Supported values:
** *`en`* -- indicates the use of English.
** *`uk`* -- indicates the use of Ukrainian.

<4> *`-u`* -- attribute indicates the updating of the Platform.
====
+
[WARNING]
====
Run the script twice if the obtained log does _NOT_ match the item xref:#update-status[].
====

[#update-status]
==== Update status

The final log shown below indicates the successful completion of the Platform update process.

image:admin:installation/aws/installation-aws-3.png[image,width=468,height=178]

If in section xref:#update-platform-installer[] the *`--rm`* option was removed, you need to: ::
+
. Execute the following command to ensure the container exited with status 0 (a container status indicating it successfully completed its work).
+
[source,bash]
----
$ docker ps --all --latest
----
+
image:admin:installation/aws/installation-aws-4.png[image,width=468,height=26]

. Delete the container with the following command:
+
[source,bash]
----
$ docker rm $(docker ps --latest -q)
----

==== Necessary steps after the update

After updating the Platform from the Installer: ::

. Go to the section xref:admin:update/overview.adoc[Update].
. Perform the necessary special steps for updating to your Platform version.
. As part of the special steps, update the xref:update/update_cluster-mgmt.adoc[Platform infrastructure components] through the Control Plane interface.

[NOTE]
====
State of additional resources ::

After completing all actions, the bastion and deployer-node can be turned off. They will not be needed until the next Platform update.
====

== Common errors during platform deployment

This section provides information on common errors that may occur during platform deployment from scratch and methods for resolving them.

=== Bootstrap machine error during OKD cluster deployment

[bootstrap-machine-issue-description]
==== Problem description

During the cluster deployment, the following error occurs:

.Bootstrap virtual machine error
----
level=error msg=Attempted to gather ClusterOperator status after installation failure: listing ClusterOperator objects: Get "https://api.<CLUSTER_URL>:6443/apis/config.openshift.io/v1/clusteroperators": dial tcp <CLUSTER_IP>:6443: connect: connection refused
level=error msg=Bootstrap failed to complete: Get "https://api.<CLUSTER_URL>:6443/version": dial tcp <CLUSTER_IP>:6443: connect: connection refused
level=error msg=Failed waiting for Kubernetes API. This error usually happens when there is a problem on the bootstrap host that prevents creating a temporary control plane.
----

This error is related to the bootstrap virtual machine and usually occurs when there is a problem on the bootstrap host that prevents creating a temporary Control Plane.

[bootstrap-machine-issue-resolving]
==== Problem resolution

. Run the command to delete the cluster, keeping the same *`--dir`* parameter.
+
.Deleting the OKD cluster
----
$ ./openshift-install destroy cluster --dir /tmp/openshift-cluster/cluster-state --log-level info
----

. Wait for the cluster to be deleted and then run the installation command again.
+
.Reinstalling the cluster
----
$ ./openshift-install create cluster --dir /tmp/openshift-cluster/cluster-state --log-level=info
----

=== Vault token error during Platform deployment

[vault-token-issue-description]
==== Problem description

During Platform deployment, at the stage of installing Vault, an error may occur where the `vault_root_token` variable returns an empty value:

image:installation/aws/installation-aws-5.png[image,width=468,height=113]

This error is related to Vault not starting successfully or some platform installation steps being skipped.

[vault-token-issue-resolving]
==== Resolution

. Open your AWS account. Find the virtual machine *`platform-vault-<CLUSTER_NAME>`*.
. Access the virtual machine using EC2 Instance Connect or SSH.
. Check the status of Vault. The *`Initialized`* parameter should be `*true*`.
+
.Check Vault status
----
$ vault status
----
+
image:installation/aws/installation-aws-6.png[image,width=468,height=182]

. If the status is different, restart Vault.
+
.Restart Vault
----
$ systemctl restart vault
----

. If this error occurred during a Platform update, check if the Vault key was copied from the previous release, as described in section xref:#platform-update-vault[].
. Try running the Platform update process again, as described in section xref:#update-platform-installer[].

=== Minio SSL certificate error during Platform deployment

[minio-ssl-certificate-issue-description]
==== Problem description

During Platform deployment, at the stage of installing Minio, the following error may occur:

image:installation/aws/installation-aws-7.png[image,width=468,height=174]

[minio-ssl-certificate-issue-resolving]
==== Resolution

. Navigate to the directory with the Installer and run the container to install the Platform with the following command:
+
.Starting the container
[source,bash]
----
$ cd ~/installer/installer-<VERSION>
$ sudo docker run -it --rm \
    --name control-plane-installer-<VERSION> \
    --user root:$(id -g) \
    --net host \
    -v $(pwd):/tmp/installer \
    --env KUBECONFIG=/tmp/installer/kubeconfig \
    --env PLATFORM_DEPLOYMENT_MODE=<DEPLOYMENT_MODE> control-plane-installer:<VERSION> bash
----

. Navigate to the required directory and set environment variables.
+
.Setting environment variables
[source,bash]
----
$ cd /tmp/installer/terraform/minio/aws
$ export AWS_ACCESS_KEY_ID=$(oc get secret/aws-creds -n kube-system -o jsonpath='{.data.aws_access_key_id}' | base64 -d)
$ export AWS_SECRET_ACCESS_KEY=$(oc get secret/aws-creds -n kube-system -o jsonpath='{.data.aws_secret_access_key}' | base64 -d)
$ export CLUSTER_NAME=$(oc get node -l node-role.kubernetes.io/master -o 'jsonpath={.items[0].metadata.annotations.machine\.openshift\.io/machine}' | sed -r 's#.*/(.*)-master.*#\1#')
$ export clusterNameShort="${CLUSTER_NAME::-6}"
$ export baseDomain=$(oc get dns cluster --no-headers -o jsonpath='{.spec.baseDomain}')
$ export route53HostedZone="${baseDomain/${clusterNameShort}./}"
----

. Delete Minio using Terraform.
+
.Deleting Minio
[source,bash]
----
$ terraform init
$ terraform destroy -var cluster_name="${clusterNameShort}" -var baseDomain="${route53HostedZone}" -auto-approve
----

. Wait for Minio to be deleted. Exit the container and try running the Platform installation process again, as described in section xref:#deploy-platform-installer-scratch[] if you are deploying the platform from scratch, or section xref:#update-platform-installer[] if you are updating the platform.

=== Error sending images to Nexus during Platform deployment

[send-images-to-nexus-issue-description]
==== Problem description

During Platform deployment, at the stage of sending images to Nexus, the following error may occur:

image:installation/aws/installation-aws-8.png[image,width=468,height=228]

This error is related to *skopeo*. This tool sends images to Nexus. If an image fails to upload within 10 minutes, skopeo starts returning an error due to timeout.

[send-images-to-nexus-issue-resolving]
==== Resolution

Perform the Platform installation from the deployer-node, as described in section xref:#deploy-additional-recources-for-okd[].
